{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presentation : Structure\n",
    "\n",
    "1. Presentation personnelle\n",
    "2. Pres. globale du Projet\n",
    "3. Intro NLP & transformers \n",
    "4. Utilisation de l'appli : récuperation du texte dans le pdf\n",
    "5. Presentation du modèle : T5 + hugging face, utilisation de modèles préentrainés\n",
    "6. Data d'entrainements : \n",
    "    a. Comment on apprend a un modele a identifier du sens dans les phrases\n",
    "    b. dictionnaires & données de finetunning\n",
    "7. Exemples d'utilisations de l'app : diff pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Presentation personnelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Presentation globale du projet\n",
    "\n",
    "Application sur smartphone de traducteur de lettres administratives, tapuscrites, pour facilité l'intégration de personnes \n",
    "\n",
    "BUT : Contenu sensible, pas d'envoie de données via internet\n",
    "\n",
    "Première version : Sur ordinateur : modèle un peu lourd, acquisition du texte via pdf scannée\n",
    "\n",
    "Amélioration :  \n",
    "- Récupération des données grâce à une photo directement prise par le smartphone \n",
    "- Modèle plus léger\n",
    "- Prévoir la langue de traduction au moment du téléchargement du modèle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Evaluation d'un modele nlp de traduction :BLEU (algorithme)\n",
    "BLEU (bilingual evaluation understudy) est un algorithme d’évaluation de la qualité du texte qui a été traduit mécaniquement d’une langue naturelle à une autre. La qualité est considérée comme la correspondance entre la production d’une machine et celle d’un humain : « plus une traduction automatique est proche d’une traduction humaine professionnelle, mieux c’est » - c’est l’idée centrale derrière BLEU. BLEU a été l’une des premières métriques à revendiquer une corrélation élevée avec les jugements humains de qualité, et reste l’une des métriques automatisées les plus populaires et les moins couteuses.\n",
    "\n",
    "Les notes sont calculées pour chaque segment traduit - généralement des phrases - en les comparant avec un ensemble de traductions de référence de bonne qualité. La moyenne de ces notes est ensuite calculée sur l’ensemble du corpus pour obtenir une estimation de la qualité globale de la traduction. L’intelligibilité ou l’exactitude grammaticale ne sont pas prises en compte.\n",
    "\n",
    "La sortie de BLEU est toujours un nombre compris entre 0 et 1, qui indique dans quelle mesure le texte candidat est similaire aux textes de référence, les valeurs plus proches de 1 représentant des textes plus similaires. Peu de traductions humaines atteindront une note de 1, car cela indiquerait que le candidat est identique à l’une des traductions de référence. Pour cette raison, il n’est pas nécessaire d’obtenir un score de 1, car il y a plus de possibilités d’appariement, l’ajout de traductions de référence supplémentaires augmentera le score BLEU.\n",
    "\n",
    "Les valeurs BLEU peuvent varier grandement en fonction des paramètres et du pré-traitement. Afin d'atténuer ce problème, l'implémentation recommandée est sacreBLEU.\n",
    "\n",
    "Il est à noter que le score BLEU est souvent rapporté en pourcentage, soit en 0 et 100%, plutôt qu'entre 0 et 1."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
