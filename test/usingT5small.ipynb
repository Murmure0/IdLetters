{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3244019631.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install transformers\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers\n",
    "# pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AutoModelForConditionalGeneration' from 'transformers' (c:\\Users\\admindc\\anaconda3\\envs\\tf_devia\\lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m T5Tokenizer, AutoModelForConditionalGeneration\n\u001b[0;32m      3\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mt5-small\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m model \u001b[39m=\u001b[39m AutoModelForConditionalGeneration\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'AutoModelForConditionalGeneration' from 'transformers' (c:\\Users\\admindc\\anaconda3\\envs\\tf_devia\\lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, AutoModelForConditionalGeneration\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "model = AutoModelForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Translate this text to French.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(\"Translate this text to French.\", return_tensors=\"tf\")\n",
    "outputs = model(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_text = tokenizer.decode(outputs.logits[0], skip_special_tokens=True)\n",
    "print(\"Translated text:\", translated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admindc\\anaconda3\\envs\\tf_devia\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:199: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac03fdad92914c0eacf5d47251b38959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admindc\\anaconda3\\envs\\tf_devia\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\admindc\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5Model\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5Model.from_pretrained(\"t5-base\")\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n",
    "\n",
    "# forward pass\n",
    "outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0005, -0.1320,  0.2328,  ...,  0.0699,  0.0568, -0.1638],\n",
       "         [-0.0052, -0.1856,  0.1215,  ..., -0.0225, -0.0012, -0.2632],\n",
       "         [ 0.0824, -0.3187,  0.0708,  ...,  0.0134,  0.0274, -0.3392],\n",
       "         [ 0.1529, -0.4006,  0.1589,  ...,  0.0711,  0.0720, -0.2287]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17cd53381ee64c659c0264a3354a8d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2882c892ee4d3a9a325f4af97e9493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_tf_bert.TFBertModel'>\n"
     ]
    }
   ],
   "source": [
    "bert_model = TFAutoModel.from_pretrained(\"bert-base-cased\")\n",
    "print(type(bert_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n"
     ]
    }
   ],
   "source": [
    "bert_config = AutoConfig.from_pretrained(\"bert-base-cased\")\n",
    "print(type(bert_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, TFBertModel\n",
    "\n",
    "bert_config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "bert_model2 = TFBertModel(bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model2.save_pretrained(\"my-bert-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "Some layers of TFBertModel were not initialized from the model checkpoint at my-bert-model and are newly initialized: ['bert/encoder/layer_._0/attention/self/query/bias:0', 'bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._10/intermediate/dense/kernel:0', 'bert/encoder/layer_._5/attention/self/key/kernel:0', 'bert/encoder/layer_._2/intermediate/dense/bias:0', 'bert/encoder/layer_._4/attention/self/key/bias:0', 'bert/encoder/layer_._0/attention/output/dense/bias:0', 'bert/encoder/layer_._6/attention/self/key/bias:0', 'bert/encoder/layer_._3/intermediate/dense/bias:0', 'bert/encoder/layer_._5/attention/self/key/bias:0', 'bert/encoder/layer_._6/intermediate/dense/kernel:0', 'bert/encoder/layer_._3/output/dense/bias:0', 'bert/encoder/layer_._4/output/LayerNorm/beta:0', 'bert/encoder/layer_._0/output/dense/kernel:0', 'bert/encoder/layer_._8/output/dense/kernel:0', 'bert/encoder/layer_._3/attention/self/value/bias:0', 'bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._1/output/dense/bias:0', 'bert/encoder/layer_._11/output/dense/kernel:0', 'bert/encoder/layer_._5/attention/self/query/bias:0', 'bert/encoder/layer_._4/attention/self/query/kernel:0', 'bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._10/attention/self/key/bias:0', 'bert/encoder/layer_._7/attention/self/key/bias:0', 'bert/encoder/layer_._5/output/LayerNorm/beta:0', 'bert/encoder/layer_._0/attention/self/value/bias:0', 'bert/encoder/layer_._10/attention/self/query/bias:0', 'bert/encoder/layer_._9/intermediate/dense/kernel:0', 'bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'bert/encoder/layer_._5/intermediate/dense/bias:0', 'bert/encoder/layer_._1/attention/output/dense/kernel:0', 'bert/encoder/layer_._2/intermediate/dense/kernel:0', 'bert/encoder/layer_._4/intermediate/dense/bias:0', 'bert/encoder/layer_._5/attention/self/value/kernel:0', 'bert/encoder/layer_._10/output/dense/kernel:0', 'bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._6/intermediate/dense/bias:0', 'bert/encoder/layer_._11/attention/self/key/bias:0', 'bert/encoder/layer_._7/attention/self/value/kernel:0', 'bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'bert/embeddings/word_embeddings/weight:0', 'bert/encoder/layer_._11/output/LayerNorm/beta:0', 'bert/encoder/layer_._8/attention/output/dense/kernel:0', 'bert/encoder/layer_._9/attention/output/dense/bias:0', 'bert/encoder/layer_._10/attention/self/value/kernel:0', 'bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._1/output/dense/kernel:0', 'bert/encoder/layer_._5/intermediate/dense/kernel:0', 'bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._1/attention/self/key/kernel:0', 'bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'bert/encoder/layer_._1/intermediate/dense/kernel:0', 'bert/encoder/layer_._0/attention/self/key/bias:0', 'bert/encoder/layer_._11/attention/output/dense/bias:0', 'bert/encoder/layer_._10/intermediate/dense/bias:0', 'bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'bert/encoder/layer_._2/output/dense/kernel:0', 'bert/encoder/layer_._6/output/dense/kernel:0', 'bert/encoder/layer_._2/attention/self/query/bias:0', 'bert/encoder/layer_._6/attention/self/key/kernel:0', 'bert/encoder/layer_._6/attention/output/dense/kernel:0', 'bert/encoder/layer_._8/attention/output/dense/bias:0', 'bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._1/intermediate/dense/bias:0', 'bert/encoder/layer_._5/output/dense/bias:0', 'bert/encoder/layer_._6/attention/self/value/bias:0', 'bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'bert/embeddings/LayerNorm/beta:0', 'bert/encoder/layer_._3/attention/self/value/kernel:0', 'bert/encoder/layer_._0/output/dense/bias:0', 'bert/encoder/layer_._1/attention/self/value/bias:0', 'bert/encoder/layer_._7/attention/self/query/bias:0', 'bert/encoder/layer_._2/attention/self/value/bias:0', 'bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._8/attention/self/query/kernel:0', 'bert/embeddings/LayerNorm/gamma:0', 'bert/encoder/layer_._7/attention/output/dense/bias:0', 'bert/encoder/layer_._11/attention/self/key/kernel:0', 'bert/encoder/layer_._8/intermediate/dense/kernel:0', 'bert/encoder/layer_._7/intermediate/dense/kernel:0', 'bert/encoder/layer_._9/output/LayerNorm/beta:0', 'bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._4/attention/self/value/kernel:0', 'bert/encoder/layer_._6/output/LayerNorm/beta:0', 'bert/encoder/layer_._9/output/dense/bias:0', 'bert/pooler/dense/kernel:0', 'bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'bert/encoder/layer_._4/attention/output/dense/bias:0', 'bert/encoder/layer_._4/output/dense/bias:0', 'bert/encoder/layer_._3/intermediate/dense/kernel:0', 'bert/encoder/layer_._6/attention/self/value/kernel:0', 'bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._11/attention/output/dense/kernel:0', 'bert/encoder/layer_._9/output/dense/kernel:0', 'bert/encoder/layer_._2/attention/self/key/kernel:0', 'bert/encoder/layer_._2/attention/output/dense/bias:0', 'bert/encoder/layer_._4/intermediate/dense/kernel:0', 'bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'bert/embeddings/position_embeddings/weight:0', 'bert/encoder/layer_._8/attention/self/value/bias:0', 'bert/encoder/layer_._11/intermediate/dense/kernel:0', 'bert/encoder/layer_._8/output/LayerNorm/beta:0', 'bert/encoder/layer_._9/attention/self/key/kernel:0', 'bert/encoder/layer_._4/attention/output/dense/kernel:0', 'bert/encoder/layer_._6/attention/self/query/bias:0', 'bert/encoder/layer_._11/intermediate/dense/bias:0', 'bert/encoder/layer_._3/attention/self/query/bias:0', 'bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'bert/encoder/layer_._11/attention/self/value/kernel:0', 'bert/encoder/layer_._3/attention/output/dense/bias:0', 'bert/encoder/layer_._0/intermediate/dense/kernel:0', 'bert/encoder/layer_._9/attention/self/query/bias:0', 'bert/encoder/layer_._7/output/LayerNorm/beta:0', 'bert/encoder/layer_._9/attention/self/value/kernel:0', 'bert/encoder/layer_._10/attention/output/dense/kernel:0', 'bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._1/output/LayerNorm/beta:0', 'bert/encoder/layer_._4/output/dense/kernel:0', 'bert/encoder/layer_._2/attention/self/key/bias:0', 'bert/encoder/layer_._3/attention/self/key/bias:0', 'bert/encoder/layer_._7/attention/output/dense/kernel:0', 'bert/encoder/layer_._9/attention/self/value/bias:0', 'bert/encoder/layer_._10/output/dense/bias:0', 'bert/pooler/dense/bias:0', 'bert/encoder/layer_._3/attention/self/key/kernel:0', 'bert/encoder/layer_._8/attention/self/value/kernel:0', 'bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'bert/encoder/layer_._1/attention/self/query/kernel:0', 'bert/encoder/layer_._3/output/LayerNorm/beta:0', 'bert/encoder/layer_._7/attention/self/value/bias:0', 'bert/encoder/layer_._2/attention/output/dense/kernel:0', 'bert/encoder/layer_._3/output/dense/kernel:0', 'bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'bert/encoder/layer_._4/attention/self/value/bias:0', 'bert/encoder/layer_._9/intermediate/dense/bias:0', 'bert/encoder/layer_._0/attention/self/query/kernel:0', 'bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._7/output/dense/bias:0', 'bert/encoder/layer_._8/attention/self/key/kernel:0', 'bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'bert/encoder/layer_._0/attention/self/value/kernel:0', 'bert/encoder/layer_._10/output/LayerNorm/beta:0', 'bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._5/attention/self/value/bias:0', 'bert/encoder/layer_._2/attention/self/query/kernel:0', 'bert/encoder/layer_._8/attention/self/query/bias:0', 'bert/encoder/layer_._11/output/dense/bias:0', 'bert/encoder/layer_._6/attention/output/dense/bias:0', 'bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'bert/encoder/layer_._1/attention/self/key/bias:0', 'bert/encoder/layer_._9/attention/output/dense/kernel:0', 'bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._9/attention/self/query/kernel:0', 'bert/encoder/layer_._10/attention/output/dense/bias:0', 'bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'bert/encoder/layer_._11/attention/self/query/kernel:0', 'bert/encoder/layer_._4/attention/self/key/kernel:0', 'bert/encoder/layer_._5/output/dense/kernel:0', 'bert/encoder/layer_._1/attention/self/value/kernel:0', 'bert/encoder/layer_._8/attention/self/key/bias:0', 'bert/encoder/layer_._3/attention/output/dense/kernel:0', 'bert/encoder/layer_._8/output/dense/bias:0', 'bert/encoder/layer_._10/attention/self/key/kernel:0', 'bert/encoder/layer_._6/attention/self/query/kernel:0', 'bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._6/output/dense/bias:0', 'bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'bert/encoder/layer_._2/output/LayerNorm/beta:0', 'bert/encoder/layer_._10/attention/self/query/kernel:0', 'bert/encoder/layer_._2/output/dense/bias:0', 'bert/encoder/layer_._1/attention/output/dense/bias:0', 'bert/encoder/layer_._1/attention/self/query/bias:0', 'bert/encoder/layer_._5/attention/output/dense/kernel:0', 'bert/embeddings/token_type_embeddings/weight:0', 'bert/encoder/layer_._0/attention/self/key/kernel:0', 'bert/encoder/layer_._0/output/LayerNorm/beta:0', 'bert/encoder/layer_._5/attention/output/dense/bias:0', 'bert/encoder/layer_._11/attention/self/query/bias:0', 'bert/encoder/layer_._3/attention/self/query/kernel:0', 'bert/encoder/layer_._4/attention/self/query/bias:0', 'bert/encoder/layer_._0/intermediate/dense/bias:0', 'bert/encoder/layer_._8/intermediate/dense/bias:0', 'bert/encoder/layer_._9/attention/self/key/bias:0', 'bert/encoder/layer_._10/attention/self/value/bias:0', 'bert/encoder/layer_._11/attention/self/value/bias:0', 'bert/encoder/layer_._7/attention/self/key/kernel:0', 'bert/encoder/layer_._2/attention/self/value/kernel:0', 'bert/encoder/layer_._7/output/dense/kernel:0', 'bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._5/attention/self/query/kernel:0', 'bert/encoder/layer_._7/intermediate/dense/bias:0', 'bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'bert/encoder/layer_._7/attention/self/query/kernel:0', 'bert/encoder/layer_._0/attention/output/dense/kernel:0', 'bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "new_bert_model = TFBertModel.from_pretrained(\"my-bert-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Impossible to guess which tokenizer to use. Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m      3\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtranslate English to French: Legumes share resources with nitrogen-fixing bacteria.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m translator \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mtranslation\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49mnew_bert_model)\n\u001b[0;32m      5\u001b[0m translator(text)\n",
      "File \u001b[1;32mc:\\Users\\admindc\\anaconda3\\envs\\tf_devia\\lib\\site-packages\\transformers\\pipelines\\__init__.py:868\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m         tokenizer \u001b[39m=\u001b[39m config\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Impossible to guess what is the right tokenizer here\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[0;32m    869\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mImpossible to guess which tokenizer to use. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    870\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPlease provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    871\u001b[0m         )\n\u001b[0;32m    873\u001b[0m \u001b[39m# Instantiate tokenizer if needed\u001b[39;00m\n\u001b[0;32m    874\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tokenizer, (\u001b[39mstr\u001b[39m, \u001b[39mtuple\u001b[39m)):\n",
      "\u001b[1;31mException\u001b[0m: Impossible to guess which tokenizer to use. Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text = \"translate English to French: Legumes share resources with nitrogen-fixing bacteria.\"\n",
    "translator = pipeline(\"translation\", model=new_bert_model)\n",
    "translator(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: '<transformers.models.bert.modeling_tf_bert.TFBertModel object at 0x000001F42D763460>'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(new_bert_model)\n\u001b[0;32m      4\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(text, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minput_ids\n",
      "File \u001b[1;32mc:\\Users\\admindc\\anaconda3\\envs\\tf_devia\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:652\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    649\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    651\u001b[0m \u001b[39m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[1;32m--> 652\u001b[0m tokenizer_config \u001b[39m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    653\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m tokenizer_config:\n\u001b[0;32m    654\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m tokenizer_config[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\admindc\\anaconda3\\envs\\tf_devia\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:496\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[39mLoads the tokenizer configuration from a pretrained model tokenizer configuration.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[39mtokenizer_config = get_tokenizer_config(\"tokenizer-test\")\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[0;32m    495\u001b[0m commit_hash \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 496\u001b[0m resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m    497\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m    498\u001b[0m     TOKENIZER_CONFIG_FILE,\n\u001b[0;32m    499\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    500\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    501\u001b[0m     resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    502\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    503\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    504\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    505\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    506\u001b[0m     subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[0;32m    507\u001b[0m     _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    508\u001b[0m     _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    509\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[0;32m    510\u001b[0m )\n\u001b[0;32m    511\u001b[0m \u001b[39mif\u001b[39;00m resolved_config_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    512\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admindc\\anaconda3\\envs\\tf_devia\\lib\\site-packages\\transformers\\utils\\hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    414\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    416\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    418\u001b[0m         path_or_repo_id,\n\u001b[0;32m    419\u001b[0m         filename,\n\u001b[0;32m    420\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    421\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[0;32m    422\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    423\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    424\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    425\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    426\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    427\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    428\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    429\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    430\u001b[0m     )\n\u001b[0;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m    433\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    434\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    435\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    436\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    437\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    438\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\admindc\\anaconda3\\envs\\tf_devia\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:110\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39mfor\u001b[39;00m arg_name, arg_value \u001b[39min\u001b[39;00m chain(\n\u001b[0;32m    106\u001b[0m     \u001b[39mzip\u001b[39m(signature\u001b[39m.\u001b[39mparameters, args),  \u001b[39m# Args values\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     kwargs\u001b[39m.\u001b[39mitems(),  \u001b[39m# Kwargs values\u001b[39;00m\n\u001b[0;32m    108\u001b[0m ):\n\u001b[0;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m arg_name \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mrepo_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfrom_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mto_id\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m--> 110\u001b[0m         validate_repo_id(arg_value)\n\u001b[0;32m    112\u001b[0m     \u001b[39melif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m arg_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m         has_token \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admindc\\anaconda3\\envs\\tf_devia\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:164\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must be in the form \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrepo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnamespace/repo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Use `repo_type` argument if needed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m REPO_ID_REGEX\u001b[39m.\u001b[39mmatch(repo_id):\n\u001b[1;32m--> 164\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    165\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must use alphanumeric chars or \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m--\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m\u001b[39m are\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    166\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m forbidden, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m cannot start or end the name, max length is 96:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    167\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    170\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m--\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m repo_id \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m repo_id:\n\u001b[0;32m    171\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot have -- or .. in repo_id: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: '<transformers.models.bert.modeling_tf_bert.TFBertModel object at 0x000001F42D763460>'."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(new_bert_model)\n",
    "inputs = tokenizer(text, return_tensors=\"tf\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM\n",
    "\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"my_awesome_opus_books_model\")\n",
    "outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_devia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
